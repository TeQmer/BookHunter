import asyncio
import re
import requests
from datetime import datetime
from typing import List, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from parsers.base import BaseParser, Book
from services.logger import parser_logger

class ChitaiGorodParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –º–∞–≥–∞–∑–∏–Ω–∞ '–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥'"""
    
    def __init__(self):
        super().__init__("chitai-gorod", delay_min=2, delay_max=4)
        self.base_url = "https://www.chitai-gorod.ru"
        
    async def search_books(self, query: str, max_pages: int = 1, limit: int = None, fetch_details: bool = False) -> List[Book]:
        """–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –Ω–∞ —Å–∞–π—Ç–µ chitai-gorod.ru
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–æ–∏—Å–∫–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–Ω–∏–≥ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            fetch_details: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏ (–¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫)
        """

        await self.log_operation("search", "info", f"–ü–æ–∏—Å–∫ –∫–Ω–∏–≥ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")

        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –¥–ª—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
        from urllib.parse import quote
        encoded_query = quote(query.encode('utf-8'))

        search_url = f"{self.base_url}/search?phrase={encoded_query}"
        await self.log_operation("search", "info", f"–§–æ—Ä–º–∏—Ä—É–µ–º URL: {search_url}")

        html_content = await self._make_request(search_url)

        if not html_content:
            await self.log_operation("search", "error", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞")
            return []

        try:
            soup = BeautifulSoup(html_content, 'lxml')
            books = self._parse_search_results(soup, fetch_details=fetch_details)

            await self.log_operation("search", "success", f"–ù–∞–π–¥–µ–Ω–æ –∫–Ω–∏–≥: {len(books)}", len(books))
            return books

        except Exception as e:
            await self.log_operation("search", "error", f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return []
    
    async def get_book_details(self, url: str) -> Optional[Book]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        
        await self.log_operation("details", "info", f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –∫–Ω–∏–≥–∏: {url}")
        
        html_content = await self._make_request(url)
        if not html_content:
            await self.log_operation("details", "error", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏")
            return None
        
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            book_data = self._parse_book_details(soup, url)
            
            if book_data:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                book_data["parsed_at"] = datetime.now()
                
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Book
                book = Book(**book_data)
                await self.log_operation("details", "success", f"–ü–æ–ª—É—á–µ–Ω—ã –¥–µ—Ç–∞–ª–∏ –∫–Ω–∏–≥–∏: {book.title}")
            else:
                await self.log_operation("details", "error", "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏")
                book = None
            
            return book
            
        except Exception as e:
            await self.log_operation("details", "error", f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None
        
    async def check_discounts(self) -> List[Book]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        
        await self.log_operation("discounts", "info", "–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ–∏—Å–∫ –∫–Ω–∏–≥ —Å –≤—ã—Å–æ–∫–∏–º–∏ —Å–∫–∏–¥–∫–∞–º–∏ —á–µ—Ä–µ–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        all_discount_books = []
        
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –∑–∞–ø—Ä–æ—Å—ã
        popular_queries = ["–∫–Ω–∏–≥–∏", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "python", "javascript", "java", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–±–∏–∑–Ω–µ—Å", "–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è"]
        
        for query in popular_queries:
            try:
                await self._random_delay()
                books = await self.search_books(query)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–Ω–∏–≥–∏ —Å–æ —Å–∫–∏–¥–∫–∞–º–∏ 15% –∏ –±–æ–ª—å—à–µ
                discount_books = [book for book in books if book.discount_percent and book.discount_percent >= 15]
                all_discount_books.extend(discount_books)
                
            except Exception as e:
                await self.log_operation("discounts", "warning", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Å–∫–∏–¥–æ—á–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
        try:
            await self._random_delay()
            # –ò—â–µ–º –∫–Ω–∏–≥–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ —Å–∫–∏–¥–æ–∫
            discount_keywords = ["-50%", "-30%", "-25%", "—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞"]
            for keyword in discount_keywords:
                books = await self.search_books(keyword)
                all_discount_books.extend(books)
                
        except Exception as e:
            await self.log_operation("discounts", "warning", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å–∫–∏–¥–æ–∫: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ source_id –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É —Å–∫–∏–¥–∫–∏
        unique_books = []
        seen_ids = set()
        for book in all_discount_books:
            if book.source_id not in seen_ids:
                unique_books.append(book)
                seen_ids.add(book.source_id)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–∏–¥–∫–∏
        unique_books.sort(key=lambda x: x.discount_percent or 0, reverse=True)
        
        await self.log_operation("discounts", "success", f"–ù–∞–π–¥–µ–Ω–æ –∞–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∫–Ω–∏–≥: {len(unique_books)}", len(unique_books))
        return unique_books
    
    def _parse_search_results(self, soup: BeautifulSoup, fetch_details: bool = False) -> List[Book]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            fetch_details: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∫–∞–∂–¥–æ–π –∫–Ω–∏–≥–∏ (–¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫)
        """
        books = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        product_links = soup.find_all('a', href=re.compile(r'/product/'))
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å—Å—ã–ª–∫–∏, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∏—Ö –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        if not product_links:
            # –ò—â–µ–º –ø–æ alt —Ç–µ–∫—Å—Ç—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            img_links = soup.find_all('img', alt=True)
            for img in img_links:
                parent_link = img.find_parent('a', href=re.compile(r'/product/'))
                if parent_link:
                    product_links.append(parent_link)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_links = []
        seen_urls = set()
        for link in product_links:
            href = link.get('href', '')
            if href not in seen_urls:
                unique_links.append(link)
                seen_urls.add(href)
        
        for link in unique_links:
            try:
                book_data = self._extract_book_data_from_link(link, fetch_details=fetch_details)
                if book_data and self.validate_book_data(book_data):
                    book_data["parsed_at"] = datetime.now()
                    book = Book(**book_data)
                    books.append(book)
                    
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏: {e}")
                continue
        
        return books
    
    def _extract_book_data_from_link(self, link, fetch_details: bool = False) -> Optional[dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏ –∏–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç

        Args:
            link: HTML —ç–ª–µ–º–µ–Ω—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–Ω–∏–≥—É
            fetch_details: –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –ø–µ—Ä–µ–ø–ª—ë—Ç, –∂–∞–Ω—Ä—ã)
        """
        try:
            book_data = {
                "source": "chitai-gorod",
                "genres": []
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∫–Ω–∏–≥—É
            product_url = urljoin(self.base_url, link.get('href', ''))
            book_data["url"] = product_url
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–Ω–∏–≥–∏ –∏–∑ URL
            url_match = re.search(r'/product/[^/]+-(\d+)', product_url)
            if url_match:
                book_data["source_id"] = url_match.group(1)
            else:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ—Ä–∞ –∏–∑ title —Å—Å—ã–ª–∫–∏
            title_text = link.get('title', '')
            if not title_text:
                return None
            
            # –ü–∞—Ä—Å–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ—Ä–∞ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "–ù–∞–∑–≤–∞–Ω–∏–µ (–ê–≤—Ç–æ—Ä)"
            if " (" in title_text and ")" in title_text:
                parts = title_text.split(" (", 1)
                book_data["title"] = parts[0].strip()
                if len(parts) > 1:
                    book_data["author"] = parts[1].replace(")", "").strip()
                else:
                    book_data["author"] = None
            else:
                book_data["title"] = title_text
                book_data["author"] = None
        
            # üî• –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –¥–µ—Ç—Å–∫–∞—è –∫–Ω–∏–≥–∞ –∏–ª–∏ –∫–æ–Ω—Ü—Ç–æ–≤–∞—Ä
            if self._is_excluded_content(book_data["title"], book_data.get("author")):
                self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–¥–µ—Ç—Å–∫–∞—è/—Ä–∞–∑–≤–∏–≤–∞—é—â–∞—è)")
                return None
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô: –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ img –≤–Ω—É—Ç—Ä–∏ —Å—Å—ã–ª–∫–∏
            img_elem = link.find('img')
            if img_elem:
                img_src = img_elem.get('src') or img_elem.get('data-src')
                if img_src:
                    cleaned_img_url = self._clean_image_url(img_src)
                    if cleaned_img_url:
                        book_data["image_url"] = cleaned_img_url
                    else:
                        # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–µ, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
                        pass
            
            # –ò—â–µ–º —Ü–µ–Ω—ã –∏ —Å–∫–∏–¥–∫–∏ –≤ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ä–æ–¥–∏—Ç–µ–ª—è —Å—Å—ã–ª–∫–∏ –∏ –∏–¥–µ–º –≤–≤–µ—Ä—Ö –ø–æ –¥–µ—Ä–µ–≤—É
            search_element = link
            price_text = ""
            
            for _ in range(3):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ 3 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                if search_element:
                    try:
                        current_text = search_element.get_text()
                        if len(current_text) > len(price_text):
                            price_text = current_text
                        search_element = search_element.parent
                    except:
                        break
                else:
                    break
        
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å–∞–º–æ–π —Å—Å—ã–ª–∫–∏
            if not price_text:
                price_text = link.get_text()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—ã
            price_matches = re.findall(r'(\d+(?:\s\xa0?\d+)*)\s*‚ÇΩ', price_text)
            if price_matches:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ü–µ–Ω–∞ - —ç—Ç–æ —Ç–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
                current_price_str = price_matches[-1].replace(' ', '').replace('\xa0', '')
                try:
                    book_data["current_price"] = float(current_price_str)
                except ValueError:
                    return None
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –µ—â–µ –æ–¥–Ω–∞ —Ü–µ–Ω–∞ - —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞
                if len(price_matches) >= 2:
                    old_price_str = price_matches[-2].replace(' ', '').replace('\xa0', '')
                    try:
                        book_data["original_price"] = float(old_price_str)
                    except ValueError:
                        pass
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–∫–∏–¥–∫—É
            discount_match = re.search(r'(-?\d+)%', price_text)
            if discount_match:
                book_data["discount_percent"] = int(discount_match.group(1))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è –∫–Ω–∏–≥–∞, –∞ –Ω–µ –¥—Ä—É–≥–æ–π —Ç–æ–≤–∞—Ä
            if not self._is_real_book(book_data):
                self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω–æ–π –∫–Ω–∏–≥–æ–π")
                return None
            
            # üî• –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–Ω–∏–≥–∏ –±–µ–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if not book_data.get("image_url"):
                self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                return None
        
            # üî• –ù–û–í–û–ï: –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            if fetch_details:
                try:
                    details_data = self._fetch_book_details(product_url)
                    if details_data:
                        # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–µ—Ç–∞–ª–∏–º–∏
                        book_data.update({
                            "publisher": details_data.get("publisher"),
                            "binding": details_data.get("binding"),
                            "isbn": details_data.get("isbn"),
                            "genres": details_data.get("genres", [])
                        })
                        self.logger.info(f"[chitai-gorod] –ò–∑–≤–ª–µ—á–µ–Ω—ã —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–Ω–∏–≥–∏ '{book_data.get('title')}': publisher={details_data.get('publisher')}, binding={details_data.get('binding')}, genres={details_data.get('genres')}")
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–µ—Ç–∞–ª–µ–π –∫–Ω–∏–≥–∏ '{book_data.get('title')}': {e}")

            return book_data if book_data.get("title") and book_data.get("current_price") else None
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∫–Ω–∏–≥–∏: {e}")
            return None
        
    def _fetch_book_details(self, url: str) -> Optional[dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫

        Args:
            url: URL –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–Ω–∏–≥–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –∫–Ω–∏–≥–∏ (publisher, binding, isbn, genres)
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
            html = self._make_request_sync(url)
            if not html:
                return None
        
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'lxml')

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ ISBN
            price_text = soup.get_text()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            characteristics = self._extract_book_characteristics(soup, price_text)

            return characteristics

        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–µ—Ç–∞–ª–µ–π –∫–Ω–∏–≥–∏ –ø–æ URL {url}: {e}")
            return None
        
    def _make_request_sync(self, url: str) -> Optional[str]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

        Returns:
            HTML —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∏–ª–∏ None
        """
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code == 200:
                return response.text
            else:
                self.logger.warning(f"HTTP {response.status_code} for {url}")
                return None

        except Exception as e:
            self.logger.warning(f"Request error for {url}: {e}")
            return None
        
    def _clean_image_url(self, img_src: str) -> Optional[str]:
        """–û—á–∏—Å—Ç–∫–∞ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        if not img_src:
            return None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
        full_url = urljoin(self.base_url, img_src)
        
        # –°–ø–∏—Å–æ–∫ fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å
        fallback_patterns = [
            'fallback-cover.webp',
            '_static/fallback-cover.webp',
            '/_static/fallback-cover.webp',
            'placeholder',
            'no-image',
            'default-cover',
            'no-cover'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ fallback –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in fallback_patterns:
            if pattern.lower() in full_url.lower():
                self.logger.debug(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {full_url}")
                return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤
        valid_patterns = [
            'product',
            'cover',
            'img-gorod',
            'pim/products',
            'cdn',
            'media',
            'images',
            'content.img-gorod.ru',
            'chitai-gorod.ru/product'
        ]
        
        # –ï—Å–ª–∏ URL –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å fallback
        has_valid_pattern = any(pattern in full_url.lower() for pattern in valid_patterns)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞–∑–º–µ—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç content.img-gorod.ru)
        if '?' in full_url and ('width=' in full_url or 'height=' in full_url):
            # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–∞–∑–º–µ—Ä–æ–≤
            if has_valid_pattern or 'content.img-gorod.ru' in full_url:
                self.logger.debug(f"–ü—Ä–∏–Ω—è—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {full_url}")
                return full_url
        
        elif not has_valid_pattern:
            self.logger.debug(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {full_url}")
            return None
        
        self.logger.debug(f"–ü—Ä–∏–Ω—è—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {full_url}")
        return full_url
    
    def _is_excluded_content(self, title: str, author: str = None) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∏—Å–∫–ª—é—á–∞–µ–º—ã–º (–¥–µ—Ç—Å–∫–∏–µ –∫–Ω–∏–≥–∏, –∫–æ–Ω—Ü—Ç–æ–≤–∞—Ä—ã –∏ —Ç.–¥.)"""
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –∞–≤—Ç–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text_to_check = f"{title} {author or ''}".lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –≤–∑—Ä–æ—Å–ª—ã—Ö
        excluded_keywords = [
            # –î–µ—Ç—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã
            '–¥–ª—è –¥–µ—Ç–µ–π', '–¥–µ—Ç—Å–∫–∞—è', '–¥–µ—Ç—Å–∫–∏–µ', '–¥–æ—à–∫–æ–ª—å–Ω–∏–∫', '–¥–æ—à–∫–æ–ª—å–Ω–∞—è', '–¥–æ—à–∫–æ–ª—å–Ω–æ–µ',
            '–º–∞–ª—ã—à', '–º–∞–ª—ã—à–∞', '—Ä–µ–±–µ–Ω–æ–∫', '–¥–µ—Ç—Å–∫–∏–π', '–¥–µ—Ç—Å–∫–æ–≥–æ', '–¥–µ—Ç—Å–∫–æ–≥–æ', '–¥–µ—Ç—Å–∫–∏—Ö',
            '–∫–Ω–∏–∂–∫–∞-–∫–∞—Ä—Ç–∏–Ω–∫–∞', '–∫–Ω–∏–∂–∫–∞ —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏', '—Ä–∞—Å–∫—Ä–∞—Å–∫–∞', '—Ä–∞—Å–∫—Ä–∞—Å–∫–∏',
            '–ø—Ä–æ–ø–∏—Å–∏', '–ø—Ä–æ–ø–∏—Å—å', '–∞–∑–±—É–∫–∞', '–±—É–∫–≤–∞—Ä—å', '—Å–ª–æ–≥', '—Å–ª–æ–≥–∏',
            '–¥–µ—Ç—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞', '–¥–µ—Ç—Å–∫–∞—è –∫–Ω–∏–≥–∞', '–¥–µ—Ç—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞',
            
            # –ò–≥—Ä—ã –∏ –∏–≥—Ä—É—à–∫–∏
            '–∏–≥—Ä–∞', '–∏–≥—Ä—ã', '–∏–≥—Ä—É—à–∫–∞', '–∏–≥—Ä—É—à–∫–∏', '–ø–∞–∑–ª', '–ø–∞–∑–ª—ã', '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä',
            '–∫—É–±–∏–∫–∏', '–º—è–≥–∫–∞—è –∏–≥—Ä—É—à–∫–∞', '–ø–ª—é—à–µ–≤—ã–π', '–ø–ª—é—à–µ–≤–∞—è', '–ø–ª—é—à–µ–≤–æ–µ',
            '–Ω–∞—Å—Ç–æ–ª—å–Ω–∞—è –∏–≥—Ä–∞', '–Ω–∞—Å—Ç–æ–ª—å–Ω—ã–µ –∏–≥—Ä—ã', '–¥–µ—Ç—Å–∫–∞—è –∏–≥—Ä–∞', '–¥–µ—Ç—Å–∫–∏–µ –∏–≥—Ä—ã',
            
            # –ö–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã
            '—Ç–µ—Ç—Ä–∞–¥—å', '—Ç–µ—Ç—Ä–∞–¥–∏', '–ø–ª–∞–Ω–Ω–µ—Ä', '–ø–ª–∞–Ω–Ω–µ—Ä—ã', '–µ–∂–µ–¥–Ω–µ–≤–Ω–∏–∫', '–µ–∂–µ–¥–Ω–µ–≤–Ω–∏–∫–∏',
            '–±–ª–æ–∫–Ω–æ—Ç', '–±–ª–æ–∫–Ω–æ—Ç—ã', '–∑–∞–ø–∏—Å–Ω–∞—è –∫–Ω–∏–∂–∫–∞', '–∑–∞–ø–∏—Å–Ω—ã–µ –∫–Ω–∏–∂–∫–∏',
            '–∫–∞–Ω—Ü—Ç–æ–≤–∞—Ä—ã', '–∫–∞–Ω—Ü–µ–ª—è—Ä—Å–∫–∏–µ —Ç–æ–≤–∞—Ä—ã', '–æ—Ñ–∏—Å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã',
            
            # –¢–æ–≤–∞—Ä—ã –¥–ª—è –º–ª–∞–¥–µ–Ω—Ü–µ–≤
            '–º–ª–∞–¥–µ–Ω–µ—Ü', '–º–ª–∞–¥–µ–Ω—Ü–∞', '–º–ª–∞–¥–µ–Ω—á–µ—Å–∫–∏–π', '–¥–ª—è –º–ª–∞–¥–µ–Ω—Ü–µ–≤',
            '–¥–µ—Ç—Å–∫–∞—è –∫—Ä–æ–≤–∞—Ç–∫–∞', '–¥–µ—Ç—Å–∫–∞—è –º–µ–±–µ–ª—å', '–¥–µ—Ç—Å–∫–∏–π —Å—Ç—É–ª',
            
            # –†–∞–∑–≤–∏–≤–∞—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è –¥–µ—Ç–µ–π
            '—Ä–∞–∑–≤–∏–≤–∞—é—â–∞—è', '—Ä–∞–∑–≤–∏–≤–∞—é—â–∏–µ', '–¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è', '–æ–±—É—á–∞—é—â–∞—è', '–æ–±—É—á–∞—é—â–∏–µ',
            '—Ä–∞–∑–≤–∏–≤–∞—é—â–∞—è –∏–≥—Ä–∞', '—Ä–∞–∑–≤–∏–≤–∞—é—â–∏–µ –∏–≥—Ä—ã', '–æ–±—É—á–∞—é—â–∞—è –∏–≥—Ä–∞', '–æ–±—É—á–∞—é—â–∏–µ –∏–≥—Ä—ã',
            '—Ä–∞–∑–≤–∏–≤–∞—é—â–∏–π', '—Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ', '—Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ', '—Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏—Å–∫–ª—é—á–∞–µ–º—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in excluded_keywords:
            if keyword in text_to_check:
                return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–º –≥—Ä—É–ø–ø–∞–º –≤ —Å–∫–æ–±–∫–∞—Ö
        age_patterns = [
            r'0\+\s*–ª–µ—Ç', r'1\+\s*–ª–µ—Ç', r'2\+\s*–ª–µ—Ç', r'3\+\s*–ª–µ—Ç', r'4\+\s*–ª–µ—Ç', r'5\+\s*–ª–µ—Ç',
            r'6\+\s*–ª–µ—Ç', r'7\+\s*–ª–µ—Ç', r'8\+\s*–ª–µ—Ç', r'9\+\s*–ª–µ—Ç', r'10\+\s*–ª–µ—Ç',
            r'0-2\s*–ª–µ—Ç', r'0-3\s*–ª–µ—Ç', r'1-3\s*–ª–µ—Ç', r'2-4\s*–ª–µ—Ç', r'3-5\s*–ª–µ—Ç',
            r'4-6\s*–ª–µ—Ç', r'5-7\s*–ª–µ—Ç', r'6-8\s*–ª–µ—Ç', r'7-9\s*–ª–µ—Ç', r'8-10\s*–ª–µ—Ç'
        ]
        
        for pattern in age_patterns:
            if re.search(pattern, text_to_check):
                return True
        
        return False
    
    def _extract_book_characteristics(self, soup: BeautifulSoup, price_text: str) -> dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫–Ω–∏–≥–∏: –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ, –ø–µ—Ä–µ–ø–ª—ë—Ç, ISBN, –∂–∞–Ω—Ä—ã"""
        characteristics = {
            "publisher": None,
            "binding": None,
            "isbn": None,
            "genres": []
        }
        
        # üî• –ù–û–í–´–ï –°–ï–õ–ï–ö–¢–û–†–´ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥

        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏–∑ itemprop="publisher"
        publisher_elem = soup.find(attrs={'itemprop': 'publisher'})
        if publisher_elem:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ç—Ä–∏–±—É—Ç content
            characteristics["publisher"] = publisher_elem.get('content')
            # –ï—Å–ª–∏ content –Ω–µ—Ç, –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç
            if not characteristics["publisher"]:
                characteristics["publisher"] = publisher_elem.get_text(strip=True)

        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–µ–ø–ª—ë—Ç –∏–∑ itemprop="bookFormat"
        binding_elem = soup.find(attrs={'itemprop': 'bookFormat'})
        if binding_elem:
            # –ò—â–µ–º span –≤–Ω—É—Ç—Ä–∏
            span = binding_elem.find('span')
            if span:
                characteristics["binding"] = span.get_text(strip=True)
            else:
                characteristics["binding"] = binding_elem.get_text(strip=True)

        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º ISBN –∏–∑ itemprop="isbn"
        isbn_elem = soup.find(attrs={'itemprop': 'isbn'})
        if isbn_elem:
            characteristics["isbn"] = isbn_elem.get('content') or isbn_elem.get_text(strip=True)

        # –ï—Å–ª–∏ ISBN –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É
        if not characteristics["isbn"]:
            isbn_match = re.search(r'ISBN\s+([0-9-]{10,20})', price_text, re.IGNORECASE)
            if isbn_match:
                characteristics["isbn"] = isbn_match.group(1)

        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –∂–∞–Ω—Ä—ã –∏–∑ breadcrumbs
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º breadcrumbs__item--link
        genre_links = soup.find_all(class_='breadcrumbs__item--link')
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç (–æ–±—ã—á–Ω–æ —ç—Ç–æ "–ì–ª–∞–≤–Ω–∞—è")
        for link in genre_links[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
            span = link.find('span')
            if span:
                genre = span.get_text(strip=True)
                if genre and genre not in ['–ì–ª–∞–≤–Ω–∞—è', '–ö–Ω–∏–≥–∏', '–ö–∞—Ç–∞–ª–æ–≥']:
                    characteristics["genres"].append(genre)

        # –ï—Å–ª–∏ –∂–∞–Ω—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ breadcrumbs, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        if not characteristics["genres"]:
            # –ò—â–µ–º –∂–∞–Ω—Ä—ã –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ /genre/
            genre_links = soup.find_all('a', href=lambda x: x and '/genre/' in x if x else False)
            for link in genre_links:
                genre = link.get_text(strip=True)
                if genre and genre not in characteristics["genres"]:
                    characteristics["genres"].append(genre)

        # üî• FALLBACK: –ï—Å–ª–∏ –Ω–æ–≤—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç–æ–¥—ã
        if not characteristics["publisher"] or not characteristics["binding"]:
            # –ò—â–µ–º –≤ —Ç–∞–±–ª–∏—Ü–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            char_table = soup.find('table', class_=re.compile(r'characteristics|char|params|specs'))
            if char_table:
                rows = char_table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True).lower()
                        value = cells[1].get_text(strip=True)

                        # –ò–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
                        if not characteristics["publisher"] and any(keyword in label for keyword in ['–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–∏–∑–¥-–≤–æ', 'publisher']):
                            characteristics["publisher"] = value

                        # –ü–µ—Ä–µ–ø–ª—ë—Ç
                        if not characteristics["binding"] and any(keyword in label for keyword in ['–ø–µ—Ä–µ–ø–ª—ë—Ç', '–æ–±–ª–æ–∂–∫–∞', 'binding', '–æ–±–ª–æ–∂–∫–∞']):
                            characteristics["binding"] = value

            # –ò—â–µ–º –≤ —Å–ø–∏—Å–∫–∞—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (dl/dt/dd)
            if not characteristics["publisher"] or not characteristics["binding"]:
                char_list = soup.find('dl', class_=re.compile(r'characteristics|char|params|specs'))
                if char_list:
                    dts = char_list.find_all('dt')
                    dds = char_list.find_all('dd')
                    for dt, dd in zip(dts, dds):
                        label = dt.get_text(strip=True).lower()
                        value = dd.get_text(strip=True)

                        # –ò–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ
                        if not characteristics["publisher"] and any(keyword in label for keyword in ['–∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–∏–∑–¥-–≤–æ', 'publisher']):
                            characteristics["publisher"] = value

                        # –ü–µ—Ä–µ–ø–ª—ë—Ç
                        if not characteristics["binding"] and any(keyword in label for keyword in ['–ø–µ—Ä–µ–ø–ª—ë—Ç', '–æ–±–ª–æ–∂–∫–∞', 'binding', '–æ–±–ª–æ–∂–∫–∞']):
                            characteristics["binding"] = value

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        if characteristics["publisher"]:
            characteristics["publisher"] = characteristics["publisher"].strip()[:255]

        if characteristics["binding"]:
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º –ø–µ—Ä–µ–ø–ª—ë—Ç–∞
            binding = characteristics["binding"].lower().strip()
            if any(k in binding for k in ['–º—è–≥–∫–∏–π', '–º—è–≥–∫', '–º—è–≥–∫–∞—è', 'soft', 'paperback']):
                characteristics["binding"] = "–ú—è–≥–∫–∏–π"
            elif any(k in binding for k in ['—Ç–≤–µ—Ä–¥—ã–π', '—Ç–≤–µ—Ä–¥', '—Ç–≤—ë—Ä–¥—ã–π', '—Ç–≤—ë—Ä–¥', 'hard', 'hardcover']):
                characteristics["binding"] = "–¢–≤–µ—Ä–¥—ã–π"
            elif any(k in binding for k in ['—Å—É–ø–µ—Ä', 'super']):
                characteristics["binding"] = "–°—É–ø–µ—Ä–æ–±–ª–æ–∂–∫–∞"
            elif any(k in binding for k in ['–∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã–π', 'integral']):
                characteristics["binding"] = "–ò–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω—ã–π"
            else:
                characteristics["binding"] = characteristics["binding"][:100]

        self.logger.info(f"[chitai-gorod] –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: publisher={characteristics['publisher']}, binding={characteristics['binding']}, isbn={characteristics['isbn']}, genres={characteristics['genres']}")

        return characteristics

    def _is_real_book(self, book_data: dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è –∫–Ω–∏–≥–∞, –∞ –Ω–µ –¥—Ä—É–≥–æ–π —Ç–æ–≤–∞—Ä"""
        
        title = book_data.get("title", "").lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —è–≤–Ω–æ –Ω–µ –∫–Ω–∏–≥–∏
        non_book_keywords = [
            '–∏–≥—Ä–∞', '–∏–≥—Ä—É—à–∫–∞', '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä', '–ø–∞–∑–ª', '–∫—É–±–∏–∫–∏', '—Ç–µ—Ç—Ä–∞–¥—å', '–±–ª–æ–∫–Ω–æ—Ç',
            '–ø–ª–∞–Ω–Ω–µ—Ä', '–µ–∂–µ–¥–Ω–µ–≤–Ω–∏–∫', '–∑–∞–ø–∏—Å–Ω–∞—è –∫–Ω–∏–∂–∫–∞', '–∫–∞–Ω—Ü—Ç–æ–≤–∞—Ä—ã', '–æ—Ñ–∏—Å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã',
            '–¥–µ—Ç—Å–∫–∞—è –º–µ–±–µ–ª—å', '–¥–µ—Ç—Å–∫–∏–π —Å—Ç—É–ª', '–∫—Ä–æ–≤–∞—Ç–∫–∞', '–∫–æ–ª—è—Å–∫–∞', '–∞–≤—Ç–æ–∫—Ä–µ—Å–ª–æ',
            '–æ–¥–µ–∂–¥–∞', '–æ–±—É–≤—å', '–∏–≥—Ä—É—à–∫–∞', '–º—è–≥–∫–∞—è –∏–≥—Ä—É—à–∫–∞', '–ø–ª—é—à–µ–≤—ã–π'
        ]
        
        for keyword in non_book_keywords:
            if keyword in title:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ü–µ–Ω–∞ —Ä–∞–∑—É–º–Ω–∞—è –¥–ª—è –∫–Ω–∏–≥–∏ (–Ω–µ —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∞—è)
        price = book_data.get("current_price", 0)
        if price < 50 or price > 5000:  # –°–ª–∏—à–∫–æ–º –¥–µ—à–µ–≤–æ –∏–ª–∏ –¥–æ—Ä–æ–≥–æ –¥–ª—è –∫–Ω–∏–≥–∏
            self.logger.debug(f"–ö–Ω–∏–≥–∞ '{title}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∞—è —Ü–µ–Ω–∞: {price}‚ÇΩ")
            return False
        
        return True
        
    def _parse_book_details(self, soup: BeautifulSoup, url: str) -> Optional[dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∫–Ω–∏–≥–∏ –∏–∑ URL
        url_match = re.search(r'/product/[^/]+-(\d+)', url)
        if not url_match:
            return None
        
        source_id = url_match.group(1)
        
        book_data = {
            "source": "chitai-gorod",
            "source_id": source_id,
            "url": url,
            "genres": []
        }
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
        title_elem = soup.find('h1') or soup.find('h2', class_=re.compile(r'title|product'))
        if title_elem:
            book_data["title"] = title_elem.get_text(strip=True)
        else:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è
            title_elem = soup.find('title')
            if title_elem:
                title_text = title_elem.get_text(strip=True)
                # –£–±–∏—Ä–∞–µ–º "–ö—É–ø–∏—Ç—å" –∏ –¥—Ä—É–≥–∏–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
                title_text = re.sub(r'^–ö—É–ø–∏—Ç—å\s+', '', title_text, flags=re.IGNORECASE)
                title_text = re.sub(r'\s+–≤\s+–ß–∏—Ç–∞–π-–≥–æ—Ä–æ–¥–µ.*$', '', title_text, flags=re.IGNORECASE)
                book_data["title"] = title_text
        
        # –ê–≤—Ç–æ—Ä
        author_elem = soup.find('a', href=re.compile(r'/author/')) or \
                     soup.find('span', class_=re.compile(r'author'))
        if author_elem:
            book_data["author"] = author_elem.get_text(strip=True)
        
        # üî• –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –¥–µ—Ç—Å–∫–∞—è –∫–Ω–∏–≥–∞ –∏–ª–∏ –∫–æ–Ω—Ü—Ç–æ–≤–∞—Ä
        if self._is_excluded_content(book_data["title"], book_data.get("author")):
            self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (–¥–µ—Ç—Å–∫–∞—è/—Ä–∞–∑–≤–∏–≤–∞—é—â–∞—è)")
            return None
        
        # –¶–µ–Ω—ã
        price_text = soup.get_text()
        price_matches = re.findall(r'(\d+(?:\s\xa0?\d+)*)\s*‚ÇΩ', price_text)
        if price_matches:
            book_data["current_price"] = float(price_matches[0].replace(' ', '').replace('\xa0', ''))
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Ç–æ—Ä–∞—è —Ü–µ–Ω–∞, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è
            if len(price_matches) > 1:
                book_data["original_price"] = float(price_matches[1].replace(' ', '').replace('\xa0', ''))
        
        # –°–∫–∏–¥–∫–∞
        discount_match = re.search(r'(-?\d+)%', price_text)
        if discount_match:
            book_data["discount_percent"] = int(discount_match.group(1))
        
        # üî• –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–û–ò–°–ö –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô: –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Å—Ç–∞—Ö
        img_src = None
        
        # 1. –ò—â–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        img_selectors = [
            'img.product-cover',
            'img[alt*="–æ–±–ª–æ–∂–∫–∞"]',
            'img[alt*="–∫–Ω–∏–≥–∞"]',
            '.product-image img',
            '.book-cover img',
            '.cover img'
        ]
        
        for selector in img_selectors:
            img_elem = soup.select_one(selector)
            if img_elem:
                img_src = img_elem.get('src') or img_elem.get('data-src')
                if img_src:
                    break
        
        # 2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ª—é–±—ã–µ img —ç–ª–µ–º–µ–Ω—Ç—ã —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        if not img_src:
            img_elems = soup.find_all('img')
            for img in img_elems:
                src = img.get('src') or img.get('data-src')
                alt = img.get('alt', '').lower()
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–±–ª–æ–∂–µ–∫
                if src and ('cover' in alt or '–æ–±–ª–æ–∂–∫' in alt or '–∫–Ω–∏–≥–∞' in alt):
                    img_src = src
                    break
        
        # 3. –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if not img_src:
            img_elems = soup.find_all('img')
            for img in img_elems:
                src = img.get('src') or img.get('data-src')
                if src and not src.endswith('fallback-cover.webp') and 'product' in src:
                    img_src = src
                    break
        
        # –û—á–∏—â–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if img_src:
            cleaned_img_url = self._clean_image_url(img_src)
            if cleaned_img_url:
                book_data["image_url"] = cleaned_img_url
            else:
                # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–µ, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ
                pass
        
        # –û–ø–∏—Å–∞–Ω–∏–µ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        description_elem = soup.find('div', class_=re.compile(r'description|annotation|about'))
        if description_elem:
            description = description_elem.get_text(strip=True)
            if description:
                book_data["description"] = description
        
        # ISBN
        isbn_match = re.search(r'ISBN[:\s]*([\d\-X]+)', price_text, re.IGNORECASE)
        if isbn_match:
            book_data["isbn"] = isbn_match.group(1)
        
        # –ñ–∞–Ω—Ä—ã
        genre_links = soup.find_all('a', href=re.compile(r'/genre/|/category/'))
        if genre_links:
            book_data["genres"] = [link.get_text(strip=True) for link in genre_links[:5]]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –ø–µ—Ä–µ–ø–ª—ë—Ç–∞ –∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        # –ò—â–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–Ω–∏–≥–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        characteristics = self._extract_book_characteristics(soup, price_text)
        book_data.update(characteristics)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è –∫–Ω–∏–≥–∞
        if not self._is_real_book(book_data):
            self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω–æ–π –∫–Ω–∏–≥–æ–π")
            return None
        
        # üî• –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–Ω–∏–≥–∏ –±–µ–∑ –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if not book_data.get("image_url"):
            self.logger.debug(f"–ö–Ω–∏–≥–∞ '{book_data.get('title')}' –∏—Å–∫–ª—é—á–µ–Ω–∞ - –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return None
        
        return book_data if book_data.get("title") and book_data.get("current_price") else None
